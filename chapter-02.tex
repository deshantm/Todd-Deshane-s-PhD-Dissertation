%  chapter-02.tex

\chapter{Related Work}
\label{sec:related}

In this chapter, we build upon and complement a rich array of related work. First, we consider the long history of virtualization, both on server-class systems, such as the IBM mainframe, and on commodity desktop systems. Lessons learned and concepts applied over the years have provided a great foundation on which this work lies. Next, we consider the long history of the field of computer security, primarily from a network and information security standpoint, and further limiting our scope to focus primarily on the factors that have a direct impact on commodity desktop computing. Then, we describe the broad spectrum of work that has combined virtualization and security techniques to address various security issues.

There is also a substantial amount of related work that shares some of the goals of our work. First, there has been lot of effort in the network security and network intrusion detection system (NIDS) space that is often complementary to our NET-VM. Second there is a body of work in the backup and recovery space that is often complementary to our FS-VM. Finally, there is a set of related work that deals with malware and host-based intrusion detection systems (HIDS) that is often complementary to our system design as a whole.

The related work and concepts with respect to malware classification will be covered in section \ref{sec:effectiveness}, since it will be more helpful to have that discussion available to evaluate effectiveness against attacks. Similarly, some of the more advanced concepts in the FS-VM related work will be included in section \ref{sec:FutureWork}. Finally, there is a growing body of work in the area of human computer interaction (HCI) as it relates to security (SEC) in the specific field of HCI-SEC that will also be addressed in section \ref{sec:FutureWork}.


\section{Virtualization}

\subsection{Virtualization Types}
There are many different types of virtualization options. A common breakdown is into the categories of emulation, full virtualization, paravirtualization, operating system level virtualization, library virtualization, and application virtualization. Emulation is when a different architecture is being created (virtualized) often in order to simulate hardware that is not available (for example, some legacy applications require old hardware) or for development on new platforms that hardware is still being developed (for example, mobile platform emulators). Emulators typically run much slower than other types of virtulization, since all of the virtualization/emulation is done is software.

Full virtualization is creating a virtualized version of the same platform (for example, x86 on x86). Full virtualization is one of the most common types of virtualization. It performs relatively well, since most of (or sometimes all of) the operation can be performed on platform being virtualized. We rely on hardware support for virtualization in order to provide full virtualization support on Xen and KVM.

Paravirtualization is when the guest operating system kernel is modified in order to support virtualization. The virtualized platform is the same and the performance of this type of virtualization is fast, since the guest is able to be made virtualization-aware.  The limitation to this type of virtualization is that the operating system kernel source must be available. We use this type of virtualization for open source operating systems running on Xen.

Operating System level virtualization (also commonly referred to as container-based), is when the virtual guests share the kernel of the host system. This is a very fast option, but limits the guest type to be the same as the base system (for example, Linux guests must run on a Linux base). Also, providing performance isolation (one guest consuming lots of resources not affected other guests) has traditionally been difficult to implement with operating system level virtualization. 

The last two types of virtualization, library and application that we mention are not used to virtualize operating system instances, but instead run at the application layer. Library virtualization is typically done to emulate an operating system or subsystem (for example, Wine provides a subset of the Win32 API to allows Windows applications to run on Linux). Finally, application virtualization provides a managed runtime environment in order to have cross platform application mobility (for example, the Java runtime environment).

\subsection{History and Evolution}
\label{sec:related-history-evolution}

Virtual machine technology, including the virtual machine monitor (VMM) or hypervisor was pioneered by IBM in the 1960s~\cite{creasy_1981}. The original IBM VMM was designed for IBM System/370 machines and has co-evolved over the years with the IBM hardware on which it runs.  The current iteration of the IBM VMM is now known as the z/VM hypervisor and runs on IBM System z (zSeries) server hardware. Other VMM software and hardware co-evolutions include IBM's pSeries hypervisors for pSeries hardware~\cite{armstrong_power5_2005,blank_p5_2005} and Sun Microsystems Logical Domains (now Oracle VM for SPARC) for SPARC hardware~\cite{sparc_architecture_2005,sparc_vm_2006}. There also exists other hardware, such as the Alpha processor, that was specifically designed to support virtualization \cite{karger_2007}. 

The history of virtual machine technology for mainstream PC platforms is an interesting one. Since x86 hardware was not originally designed to be virtualizable~\cite{popek_1974}, this introduced additional overhead and complexity in virtualization. Also, early personal computers were not typically configured with sufficient memory to support multiple, simultaneously running VMs.  As PCs increased in power and memory prices fell, virtualization became more feasible for commodity platforms and a number of commercial and open source virtualization products were introduced. The Disco project~\cite{bugnion_1997} was the first to create a VMM that ran on experimental commodity ccNUMA hardware. Members of the Disco team later founded VMware, which is the commercial pioneer of virtual machine technology on x86 hardware~\cite{vmware_website,adams_2006}.
 
In this dissertation, we focus on two types of virtual machine technology: (1) Paravirtualization, which requires minor modifications to an operating system, making it aware that there is an underlying VMM and (2) Hardware-assisted full virtualization, which allows running unmodified operating systems on top of the VMM.

The first approach, paravirtualization, a term first coined by the developers of Denali~\cite{whitaker_2002} and then popularized by Xen~\cite{barham_2003}, brought with it evidence that virtualization benefits could be achieved with low overhead. With full virtualization, the guest VM is unaware that it is running on simulated hardware because the interface presented is the same as the physical hardware. With paravirtualization, however, the guest VM is aware that it is being virtualized since it is modified to make system (or hyper) calls directly into the hypervisor. The paravirtual modifications are usually small and are intended to improve performance by avoiding the use of the non-virtualizable instructions~\cite{popek_1974} and optimizing expensive operations. The paravirtualization approach has the advantage of better performance, but since some modification to the guest is required, it is ill-suited for use with closed source operating systems. When Xen released their performance numbers at SOSP 2003, a team of us at Clarkson University published independent verification of these results and extended the comparison to Linux running on an IBM zServer, we demonstrated that virtualization benefits could be realized on older hardware with low performance overhead~\cite{clark_2004}.
 
The second approach, hardware-assisted full virtualization, first showed up for commodity hardware in 2005 in the form of the Intel's VT-x virtualization extension, which was followed shortly after by AMD's AMD-V virtualization extensions~\cite{van_Doorn_2006}. This marked the beginning of a new era of virtualization software and hardware co-evolution. This co-evolution era, which we are still in the midst of, involves the cooperation of commodity market virtualization players such as the developers of software hypervisors (such as Xen, KVM, and VMware) and hardware vendors (such as Intel and AMD). These virtualization hardware extensions allow for unmodified guest operating systems to run more effectively on a wider variety of virtualization platforms, such as Xen and KVM. The hardware extensions are required for full virtualization support (for example, Windows guests) on Xen and is a requirement to use the Linux Kernel-based Virtual Machine (KVM)~\cite{kvm_ols07}.

First generation hardware support for virtualization (VT-x and AMD-V) made proper virtualization\cite{popek_1974} of the x86 hardware possible, but it did not always achieve performance gains compared to existing software approaches (such as binary re-writing) to virtualize the x86 architecture\cite{adams_2006}. The virtualization software and hardware co-evolution had only just begun. In an effort to shed some light on the initial mediocre performance of x86 hardware virtualization, Karger\cite{karger_2007} described some performance and security lessons learned from virtualizating the Alpha processor and compared that architecture to x86 virtualization hardware. The Alpha processor, which is based on a reduced instruction set computing (RISC) architecture, was specifically designed to support virtualization. This architecture had advantages, such as the way it handled sensitive instructions, page tables, and translation lookaside buffer (TLB) misses, which made it easier to implement high performance support for virtualization. Karger suggests that Intel and AMD should learn from the lessons of this and other architectures that were designed to support virtualization. The Karger paper and the general history of virtualization suggest that the co-evolution of hardware and software virtualization is a process that often needs to be refined over time (like, for example, the IBM z/VM and z Series) and that it is difficult for the hardware to be designed to support high performance virtualization from the beginning.

The software hypervisors, such as Xen and KVM, are evolving to make better use of the x86 hardware virtualization extensions. At the same time, the x86 hardware is evolving and vendors, such as Intel and AMD, have released second and third generation virtualization hardware extensions to add performance and security benefits. Second generation hardware extensions target performance improvements for switching between guest operation systems by adding hardware support for handling guest page tables (also referred to as shadow page tables). The specific technologies released are Intel Extended Page Tables (EPT) and AMD Nested Page Tables (NPT). Third generation hardware extensions, in the form of input/output memory management units (IOMMUs), seek to improve the security of virtual device direct memory access (DMA) and the performance of virtual I/O devices, such as graphics, disk, and network. Specific IOMMU hardware releases include Intel's VT-d and AMD's IOMMU. Other hardware virtualization technologies include Intel vPro and AMD DASH, which use on chip management capabilities and trusted platform module (TPM) technology to provide various security and manageability opportunities.

The overall virtualization hardware and software co-evolution process is making virtualization a ubiquitous part of commodity computing both in the server and desktop markets. Further evidence that virtualization is making an impact on a wider audience is the XP mode feature that was added to Windows 7. This feature uses virtual machine technology to run Windows XP applications or a full Windows XP environment inside of Windows 7\cite{windows_xp_mode}. 

\subsection{Virtual Appliances}
\label{VirtualAppliances}

A more recent trend in the virtualization space is toward virtual machine appliances (or simply virtual appliances). Virtual appliances are pre-configured virtual machine instances that are designed for specific tasks. For example, appliances exist for user-level software, such as browsers, and server software, such as web and database servers. The ability to quickly deploy a pre-configured virtual appliance is a clear and compelling advantage of virtualization and is becoming an increasingly popular method for software distribution.

Virtual appliances, at a high level, are analogous to household appliances that are used for one particular task. An even better analogy for virtual appliances is a comparison to information appliances. The term information appliance was coined by Jeff Raskin and was further described in the book ``The Invisible Computer'' by Don Norman\cite{norman_1999}. The basic idea of information appliances is that the Personal Computer (PC) is a general purpose device and since it tries to be everything to everyone, it fails at being usable. Norman's purposed solution is to replace the PC with information appliances, or single purpose devices, such as digital cameras, printers, document writers, etc. that each do one job and do it well. He argues that special-purpose devices can be made more usable. Information appliances together would then make up all the functions of the PC and the computer itself would become invisible (behind the scenes). Computers already play this role in part, but getting the computer industry to make the last big leap to a world of information appliances is a challenging one. ``The Invisible Computer'' goes into many aspects of the problem, from the market and business side of things to the complexities of large programming projects and operating systems.

Sapuntzakis, et al., first introduced the concept of a virtual appliance\cite{sapuntzakis_2003}, which they described as a virtual machine that replaces a physical computer appliance (such as a firewall). Their vision for virtual appliances was in the ``Collective'' architecture, which they described as a compute utility that provides virtual appliances as a service. Further, they explained that the virtual appliances would send their displays to a remote display on a thin client. Their concept is basically what we know of today as a cloud service that provides load balancing of infrastructure as a service (IaaS) or perhaps more closely analogous to desktop as a service (DaaS).

Virtual appliances have been a key component of architectures developed at Clarkson. For example, virtual computer appliances were mentioned as a component of the architecture proposed by Evanchik~\cite{evanchik_thesis_2004}. This was followed shortly by work in which we described how virtual machine appliances fit into a Rapid Recovery Desktop System\cite{rapid_recovery_paper_05}. The virtual machine appliance concept as proposed in our paper described creating virtual appliances by placing one or more applications that have similar data and network access needs into a virtual machine. Further we recommended that appliances come with a virtual machine contract that explicitly specifies those needs.

VMware further popularized the virtual appliance concept with marketing and virtual appliance development contests with large cash prizes~\cite{herrod_keynote_2006,vmware_appliances_website}. Many other open source and commercial vendors are also distributing virtual appliances~\cite{rPath_website, stacklet_website, virtual_appliances_website, jumpbox_website}. The associated virtual machine contracts have not gained as much traction however. Virtual machines, and therefore also virtual appliances, often come with configuration files that specify the basic hardware needs (CPU, memory, disk, etc.) of the virtual machine, but there is still a need for contracts that allow for the configuration of more fine-grained data and network resource access needs. This dissertation presents a basic, extensible contract system implementation in order to address this need.

\subsection{Virtual Machine Contracts}
\label{sec:VirtualMachineContracts}

To the best of our knowledge, the concept of virtual machine contracts (VMCs) in the context of virtual appliances was first developed at Clarkson. The VMCs put forth in Evanchik's masters thesis\cite{evanchik_thesis_2004} were based on the concept of having a virtual appliance specify a set of very specific system calls that it would be allowed to make. For example, any read or write system calls to files or directories would need to be specified. Further, network-based system calls, such as bind and listen, would need to be specified in the virtual appliance contract. The contract methodology of explicit allow and default deny is an approach that this dissertation builds upon. The contract enforcement proposed in that thesis was based on modifying the kernel of the virtual appliance and replacing system calls with hypercalls (system calls into the hypervisor) that are intercepted and validated by a contract enforcement element running in the hypervisor. 

Differences between that work and this dissertation are the contract specification and enforcement aspect. In this dissertation, we implement the contract system in a much more general and effective way, such that enforcement can occur outside of the virtual appliance where it is harder for attackers to subvert. We are not limited to relying on enforcement within the kernel of the guest VM, as is proposed in the architecture proposed by Evanchik\cite{evanchik_thesis_2004}. We also developed OSCKAR to give more flexibility and control to the virtual appliance and enforcement element designers. For example, any type of virtual appliance contract rules can be specified as long as there is an enforcement element that can respond to them. System call-based contracts could be employed with our OSCKAR system (provided that appropriate enforcement elements are implemented), but that contract style is not used in our current Rapid Recovery Desktop implementation. Chapters 3 and 4 present the design and implementation details of OSCKAR.

In ``Data Protection and Rapid Recovery From Attack With A Virtual Private File Server and Virtual Machine Appliances''\cite{rapid_recovery_paper_05}, our focus of the virtual machine contracts was on file system contract rules in which a dedicated file server virtual machine (FS-VM) stored user data and allowed virtual appliance to mount specific portions of the data in read, write, or append-only fashion. The FS-VM in that paper supported read and write rate-limiting with a modified NFS server. Here, we extend the work done in that paper to add two new components, the OSCKAR virtualization security framework and the NET-VM.

In\cite{virtual_machine_contract_ICAC09}, Matthews, et al., proposed a contract system and architecture, including the concept of enforcement elements, very similar to the architecture presented in this paper. In that paper, they demonstrate the feasibility and approach of such a system in a data center environment and described extending the Open Virtualization Format (OVF)\cite{dmtf_newsletter}, which is an open standard for packaging and distributing virtual appliances, to support more advanced data and network access rules. We extend that work to a Rapid Recovery Desktop system and implement an open source virtualization security framework that supports custom contract rules and enforcement elements, which could include support for the OVF standard in the future.

\section{Security}

The security principles employed in this dissertation have been well-studied and applied in general, but we suggest using them, in combination with other technologies, such as virtualization, in a way that it not commonly seen in practice today. Specifically, we apply the principle of least privilege, isolation, and access control to virtual appliances.

\subsection{The Principle of Least Privilege}

A number of security principles were first formally describe by Saltzer and Schroeder in\cite{saltzer_1975}. Among those principles was the principle of least privilege, which states that ``Every program and every user of the system should operate using the least set of privileges necessary to complete the job''. Saltzer and Schroeder explain that the rationale behind this principle is to limit the damage that can occur from an accident or error, to limit the interaction among privileged programs, and to provide a rationale for where to place protection mechanisms. The goal of the system described in this dissertation is that virtual appliances adhere to the principle of least privilege. Virtual appliances are an effective and practical way to implement the principle of least privilege. We are not arguing that our design perfectly achieves least privilege. Doing so would require perfect virtual appliance contracts, a very detail-level processing of virtual appliance operations, and would likely be intolerably hard to use for any user. However, as we will describe in the sections that follow, access control methods that attempt to apply the principle of least privilege to various degrees are often disabled by users. The fact that perfect adherence may not be possible is no excuse not to apply reasonable constraints on virtual appliances. Some examples of mechanisms that help to apply the principle of least privilege include isolation and access control, which we discuss in the next sections.

% discussion of these systems might be useful here
% histar, asbestos, flume etc.

\subsection{Isolation}

Complete isolation, as described in\cite{saltzer_1975}, is ``a protection system that separates principals into compartments between which no flow of information or control is possible''. Two approaches for achieving complete isolation described by Saltzer and Schroeder include isolated virtual machines and authentication mechanisms. Virtual machines, as described earlier in this chapter, have been used for many years on mainframe hardware, but until the Disco project~\cite{bugnion_1997} in 1997 were considered computationally prohibitive on commodity systems. So, traditional isolation has relied on authentication mechanisms, such as username and password system login. This dissertation makes use of virtual machines to provide isolation between applications stored in virtual appliances. We are certainly not the first to make use of virtualization for this purpose, but are among a growing list of systems using virtualization for security purposes. Examples of other research systems will be described later in this chapter. 

Although using virtual machines to provide isolation is very common in research, it is more challenging to apply virtualization to real production systems, especially on the desktop. As part of this dissertation we hope to encourage taking research ideas and converting them into real systems. This concept is exemplified by a recent alpha release of the Qubes operating system\cite{qubes-os_2010}, which is a new operating system based on the Xen hypervisor. We will describe Qubes in more detail in the Virtualization and Security section of this chapter, but for now we note that Qubes uses virtual machines and various virtualization hardware extensions to provide isolation for applications.

\subsection{Access Control}

\subsubsection{Discretionary Access Control}

Early mechanisms for access control were described by Lampson in\cite{lampson_accesscontrol_1974}. The principles and mechanisms described in that paper provide the foundation for the discretionary access control (DAC)\cite{sandhu_dac_1994} that is in common use today. The basic idea of DAC is an access control matrix with domains (users, groups, etc.) labeling the rows, objects (files, directories, processes, etc.) labeling the columns, and capabilities or access permissions (read, write, execute, etc.) as the entries within the matrix. The typically implementation is done with access control lists. One major weakness with DAC is that the granularity of access is too coarse. More specifically, if a user has access to a file, then any program running as that user has access to that file. Our approach to mitigating this problem is to apply access control at the virtual appliance level, specifically with the explicit allow, default deny policy (as was described in\cite{evanchik_thesis_2004}).

\subsubsection{Mandatory Access Control}
\label{sec:mac}

Recognizing the limitations of DAC are not a new revelation. One common alternative to DAC is mandatory access control (MAC), which is sometimes referred to as rule-based access control\cite{lindqvist_mac_2006} or lattice-based access control\cite{denning_mac_1976}. A traditional view of MAC associates it with multi-level security (MLS), but it has been recognized that the MLS-based approach is too limiting to meet many security requirements\cite{loscocco_2001}. The basic idea behind MAC is that interactions between subjects (users, programs, etc.) and objects (files, programs, etc) are handled by a set of system-wide security policies. The basic implementation is usually that all subjects and objects are labeled and policy logic is separated from the enforcement mechanism. MAC is significantly more sophisticated than DAC, but at a higher cost of complexity. 

The contract system presented in this dissertation shares many of the goals of MAC (for example, limiting user and application access, and separating mechanism from policy), but since our system is designed and implemented at the virtualization layer and applied to virtual appliances, we are able to specify resource restrictions at a relatively high level of abstraction. For example, we are able to write contract rules in terms of the virtual CPUs, memory, disks, and network resources of the virtual appliance. MAC policy, on the other hand, is typically specified in terms of lower level constructs, such as system calls and operation system objects (i.e. files and processes). Further, virtual appliances are likely to provide more isolation than MAC, since malware could potentially disable MAC that is running on a traditional operating system. However, malware within a virtual appliance would not be able to turn off our contract system unless it was somehow able to subvert the virtual machine by, for example, breaking out of the VM and into the hypervisor or breaking into a component of our trusted computing base. 

It is also worth mentioning the various implementations of MAC found in practice. These include SELinux~\cite{smalley_2001, loscocco_2001}, and AppArmor~\cite{AppArmor_2006}. Microsoft has also added a MAC model into its operating systems with the addition of Mandatory Integrity Control starting in Windows Vista~\cite{windows_integrity_mechanism, mandatory_integrity_control}. Another interesting policy enforcement tool, called Systrace\cite{provos_2003}, is touted as a lightweight replacement for MAC. This tool generates system call signatures in a learning mode and then enforces those policies in real time. A tool such as this could be used to generate system call-based contracts for applications. The output of such a tool could have been used directly with the implementation proposed in\cite{evanchik_thesis_2004}. Finally, SELinux Sandboxes~\cite{selinux_sandboxes_walsh_2009, selinux_sandboxes_morris_2009}, based on SELinux, are an attempt to further limit applications by making them run in a temporary sandbox directory that is cleaned after the application exits. SELinux-sandboxed applications can also be run within their own X server environment. 

Although MAC systems are becoming more powerful and easier to use, the most advanced features that they provide, such as SELinux Sandboxes, are generally only available for Linux applications. Using virtualization, as is described in this dissertation, allows applications from other operating systems, such as Windows, to be supported. MAC concepts and policies should also be considered complementary to our system for two reasons. First, existing MAC application policy rules (for example, the application-specific confinement rules that are written for existing MAC systems) could be used to help virtual appliance designers build better virtual appliance contracts. Second, MAC support could also be added to the virtualization layer of our system, the technologies that enabled this, sVirt\cite{sVirt_website} and XSM\cite{xsm_xen_summit_3rd}, will be discussed in the next section.

\section{Virtualization and Security}

There is a vast amount of related work that attempts to apply virtualization techniques to solve security problems. The VMM layer can be used to monitor the guest from below and, often times, without the guest OS knowing it is being watched\footnote{The ``red pill'' program can be used to detect if you are running in a virtual machine, see: ttp://invisiblethings.org/papers/redpill.html}. Some popular applications that make use of this unique perspective are intrusion detection systems~\cite{Panorama_07,VNIDA_08,VMFence_09,Psyco-Virt_07,Protecting_host_detectors_06,HyperSpector05, VMI_IDS_2003}, fault tolerance systems \cite{bressoud_1995}, virtual machine record and playback systems~\cite{dunlap_2002,king_2003}, malware analysis tools\cite{leet09_malware}, honeypots\cite{asrigo_2006}, secure desktop systems\cite{zhao_2005, Meushaw_2000, qubes-os_2010}, trusted computing platforms\cite{garfinkel_2003}, and sandboxes\cite{isolated_execution_2010}.

Some of the early work that applied virtualization to security include the following: Bressoud and Schneider developed fault-tolerant systems using virtual machine technology to replicate the state of a primary system to a backup system\cite{bressoud_1995}. Dunlap, et al., used virtual machines to provide secure logging and replay\cite{dunlap_2002}. King and Chen used virtual machine technology and secure logging to determine the cause of an attack after it had occurred\cite{king_2003}. Reed et al. used virtual machine technology to bring untrusted code safely into a shared computing environment\cite{reed_1999}. Zhao et al. used virtual machines to provide protection against root kits\cite{zhao_2005}.

\subsection{Virtualization and Isolation}

In this section we highlight two systems that use virtualization for isolation of applications. The first system, called Isolated Execution\cite{isolated_execution_2010}, which has been released by Intel in alpha form as an open source sandbox system that allows a user to right click on a binary executable file and run it in a sandbox VM. Although the Isolated Execution system is in an early development stage, it does demonstrate useful concepts that could be applied to the system described in this dissertation.

The second system is an operating system, called Qubes\cite{qubes-os_2010}, that is built on top of the Xen hypervisor. At a high level, Qubes shares many of the components of our Rapid Recovery Desktop system. Specifically, they include a network domain, which is similar to our NET-VM component, and a storage domain, which is similar to our FS-VM component. The overall goal of their system, like ours, is to isolate desktop applications from each other using virtual appliances. However, their approach differs from ours is several interesting ways. First, their virtual appliances, which they refer to as AppVMs, are assumed to be based on a common base file system so as to be able to make use of a set of read-only core system files, which has the benefit of being able to do updates once to that shared system core. Due to this architectural choice, the current release only supports Linux as the base, but they are investigating ways to support other operating systems, such as Windows, in the future. Architecturally we choose to make a different choice for our Rapid Recovery Desktop system. By creating virtual appliances that store their own system state, we are able to more easily support a variety of base operating systems.

Another difference in the Qubes architecture is that AppVMs store user data within the AppVMs themselves, which is in contrast to our Rapid Recovery Desktop system that stores user data in a dedicated FS-VM. This design choice exemplifies the different approaches in terms of recovery and threat model between the Qubes system and our Rapid Recovery Desktop system. The Qubes system uses the storage domain to store and backup user and application data in an encrypted file system, thus treating the storage domain as an untrusted entity and not part of the trusted computing base. Our system, on the other hand, uses the FS-VM to store user data and allows virtual appliances to mount specific parts of it. In this way, we treat our FS-VM as a part of our trusted computing base and do file system enforcement and protection outside of the virtual appliance in order to provide an easy way to roll back virtual appliances without affecting user data. Their threat model is specifically based around reducing the trusting computing base (they apply the concepts of disaggregation of the Xen management domain as described in\cite{murray_2008}), so that malware that compromises a particular component is not able to affect other parts of the system. In contrast, our threat model is based on distrusting virtual appliances, so that malware that compromises a virtual appliance is not able to compromise other appliances nor user data that is stored in a isolated, hardened, and carefully protected FS-VM. In Chapter 3, we will describe the methods and architectural decisions we use to protect our FS-VM.

Similar to the storage domain in the Qubes architecture, the network domain is removed from the trusted computing base of their architecture and network policy enforcement is done within each of the AppVMs. Their reasoning for this goes back to their overall threat model concept of reducing the size of the trusted computing base and the assumption that having an external network component cannot provide additional security to a compromised AppVM. As before, our Rapid Recovery Desktop system architecture is in direct contrast to theirs in that we treat our NET-VM component as part of the trusted computing base and by placing it outside of the virtual appliances we use it to protect against malicious network activity, even in the case that a virtual appliance is compromised. We believe that by distrusting the virtual appliances, we can limit their ability to do harm to the rest of the system and the rest of the world. 

A final difference between the Qubes architecture and ours is that theirs relies on hardware support for virtualization. Specifically, they make use of the IOMMU support to give direct access to the network card to their network domain and the storage controller to their storage domain. Further they make use of the Trusted eXecution Technology (TXT) and the trusted platorm module (TPM) included in Intel's vPro to do crypographic signing of of boot and disk images. We plan to make use of the IOMMU capabilities for our NET-VM and FS-VM to improve performance and security, but we do not strictly rely on them to complete our threat model like Qubes does. This difference allows our system to be deployable on more hardware than Qubes.

Despite the differences in architecture between Qubes and our Rapid Recovery Desktop, there are still ways that we could make use of some of their techniques for specific use cases. We will describe aspects of the Qubes architecture that we would like to integrate in section \ref{sec:FutureWork}.

\subsection{Virtualization and Access Control}

In the Mandatory Access Control (MAC) section earlier in this chapter, we mentioned that MAC could be added to the virtualization layer. One interesting approach taken by Quynh et al.\cite{Quynh_2006} in their VMAC system was to add a special service VM that provides central management of MAC policies for other virtual machines. A VM like the one in that paper might be able to integrated, as future work, into our Rapid Recovery Desktop system.

MAC was added to the Xen hypervisor in the form of Xen Security Modules (XSM)\cite{xsm_xen_summit_3rd,xsm_xen_summit_4th}, which were implemented by the National Information Assurance Research Lab within the National Security Agency (NSA). XSM provide various MAC policies to be enforced at the Xen hypervisor level. MAC has also been integrated into the libvirt virtualization toolkit\cite{libvirt_website} in the form of sVirt\cite{sVirt_website}. As will be described in Chapter 4 on implementation, libvirt is used to interact with the various virtualization capabilities on Linux (and other OSes).  sVirt allows for MAC policy enforcement for the various virtualization systems that run on Linux, which does not yet (and may not necessarily ever completely) include support for Xen, since Xen is a stand alone hypervisor that is not intended to be integrated into Linux itself.

MAC policies at the hypervisor level could allow for much of the basic enforcement that our Rapid Recovery Desktop system needs along with various other more complicated scenarios. For example, it could be used to assign labels to VMs and enforce various policies, such as VM A is only allowed to run if VM B is not running, at the hypervisor level. Adding MAC support at the hypervisor level of the Rapid Recovery Desktop could be an interesting area of future work.

\section{Backup and Recovery}

Our Rapid Recovery Desktop system is not intended to be a replacement for making backups, but instead it should be considered complementary. Having backups is still required in the case of hardware failure, for example. In this section, we consider the relationship of backup and recovery systems to our Rapid Recovery Desktop system. With our Rapid Recovery Desktop system, we focus on the problems of rapid system restoration and protection of user data. We are unaware of another system that has separated user data and system data in the way that we are proposing. We optimize the handling of each to provide rapid system restoration after an attack. 

Our system also helps streamline the backup process by allowing efforts to focus on the irreplaceable personal data rather than on the recoverable system data. This allows backup efforts to be customized to the differing needs of system data and personal data. The differing rates of change for system data and user data imply different backup needs for each of these data types. Specifically, there is a mismatch between the overall rate of change in system data and the user-visible rate of change. 
 
System data changes at clearly predictable points (for example, when a new application is installed or a patch is applied). Between these points, new system data may be written (such as system logs), but often this activity is of little interest to users as long as the system continues to function. For example, if a month's worth of system logs were lost, most users would be perfectly happy as long as the system was returned to an internally consistent and functioning state. Therefore, there is little need to protect this new system data between change points.
 
With user data, however, even small changes are important. For example, a user may only add 1 page of text to a report in an 8 hour workday, but the loss of that one day of data would be immediately visible. This means that efforts to protect user data can be effective even if targeted at a small percentage of overall data. Users also tend to retain a large body of personal data that is not actively being changed. Incremental backups can be kept much smaller when focused on changes to user data rather than system data.

One common approach to providing data protection and recovery from attack is making full backups of all data on the physical machine – both personal and system data. There are several ways to backup a system including copying all files to alternate media that can be mounted as a separate file system (for example, a data DVD) or making an exact bootable image of the drive with a utility such as Clonezilla\cite{clonezilla_website}.
 
Burning data to DVD or other removable media creates a portable backup that is well-suited to restoring personal data and transporting it to other systems. Mounting the backup is also an easy way to verify its correctness and completeness. However, backups of this type are rarely bootable and typically require system state to be restored via re-installation of the operating system and applications. For example, even if all of the files associated with a program are backed up, the program may still not run correctly from the backup (for example, if it requires registry changes, specific shared libraries or kernel support).
 
Making an exact image of the drive with a utility such as Clonezilla is a better way to backup system data. It maintains all dependencies between executables and the operating system. Images such as this can typically be either booted directly or used to re-image the damaged system to a bootable state. However, images such as this are not always portable to other systems as they may contain dependencies on the hardware configuration (such as CPU architecture). They are also not as convenient for mounting on other systems to extract individual files or to verify the completeness of the backup. In contrast, backing up virtual appliances makes it easier to test backups without disturbing the system state.
 
Despite the limitations of backup facilities, our system is designed to complement rather than replace backup. One goal of our system is to avoid the need for restoration from backup by preventing damage to personal data and providing rapid recovery of system data from known-good checkpoints. While it is still important to make backups, in many cases using our system's built in features can mean that users do not need to make use of their backups as often. Restoring a system from backups is often a cumbersome and manual process – not to mention an error-prone one. Given the small percentage of users that regularly backup their system (and the even smaller percentage that test the correctness of their backups), it is important to reduce the number of situations in which restoring from backup is required.
 
Our virtual machine appliances also make backups of system data that are portable to other machines. System data is made portable by the checkpoints of the virtual machine appliances. The virtualization system handles abstracting details of the underlying hardware platform so that guests will run on any machine.
 
When restoring a traditional system from a backup, users are typically forced to choose between returning their system to a usable state immediately or preserving the corrupted system for analysis of the failure or attack and to possibly recover data. With our architecture, users can save the corrupted system image while still immediately restoring a functional image. These images are also much smaller than full backups because they contain only system data, not personal data, such as a user's music collection.
 
A key advantage of our system relative to backups is that our architecture allows compromised virtual machines to be restarted automatically and almost instantaneously. As soon as an intrusion is detected, the system can be restored to an uncompromised, fully functional system very quickly. Similar advantages can be achieved with network booting facilities, such as PXE, or system reset facilities like DeepFreeze\cite{deepfreeze_website}, especially if used in conjunction with personal data mounted from a separate physical file server. However, these solutions require access to server machines – the file server, the boot server that supplies new system images, the firewall, etc. In many ways, our architecture can be viewed as bringing the advantages of a managed LAN architecture with multiple machines to a single PC environment.

Our Rapid Recovery Desktop system complements backup by making it easier to find the important user data that should be backed up, since it is all stored in the FS-VM. Similarly, updates to virtual appliance state can be carefully checkpointed and backed up when crucial changes are made and rolled back to known good backups when compromised by malware. It is also the case that system restore facilities, such as Windows System Restore\cite{windows_system_restore}, could still be used within the virtual appliances, but being able to restore a virtual appliance to a known good state with the Rapid Recovery Desktop system infrastructure may make some restore facilities obsolete. Restoring to an earlier trusted snapshot of a virtual machine is much quicker and easier than using a restore or reset facility.

\section{Network Security and Intrusion Detection}

Similar to how backup and recovery systems are complementary to our Rapid Recovery Desktop system, standard network security and intrusion detection software can complement our system.

One type of network software that would likely make a lot of sense to integrate is a network intrusion detection system (NIDS), such as Snort\cite{roesch_1999} or Bro\cite{paxson_1999}. A NIDS can be configured to watch for attack signatures and other patterns. The main limitation of such a NIDS is that new attacks, unless they contain a signature that can be recognized as malicious or follow a well known pattern, will not be noticed. Despite this limitation, a NIDS is an important component to integrate. In the Malware Classification section of Chapter 5 on evaluation, we will describe scenarios in which a NIDS could be helpful.

On the other hand, a traditional firewall might not be as necessary to integrate. Our NET-VM component can already block unknown flows from outside of the VMs at the virtual switch level. A firewall component, such as iptables\cite{iptables_website}, could be used either directly at the virtual network switch layer or on individual virtual machines for defense in depth. 

Another advantage of our flexible architecture is that enforcement elements, such as our NET-VM and other more traditional network enforcement (for example intrusion detection systems and firewalls) can be place either inside of our infrastructure or could be placed outside of our infrastructure in a separate server. The enforcement element could even be provided by a stand-alone physical network appliance. The design details that allow for this type of flexibility will be discussed in more detail in the next chapter.

\section{Anti-virus Software and Host-based Intrusion Detection Systems}

Anti-Malware software (anti-virus software) and Host-based Intrusion Detection Systems (HIDSes) can be used to complement the Rapid Recovery Desktop system as a whole. As we will describe in the Malware Classification section of Chapter 5 on evaluation, our system is able to defend quite well against a variety of attacks. However, there are still cases where a anti-virus package and an HIDS could play a significant role. The advantage of virtualization is that anti-virus software and an HIDS can be run outside of the individual virtual machines, so that malware would not be able to disable them even if it was able to obtain administrative privileges on the virtual appliance. Further, running anti-virus software or an HIDS on each virtual appliance would be a waste of resources. The argument for applying defense in depth (or having multiple levels of protection) could be used, but since file systems can be mounted privately over the virtual network and the memory can be scanned from the hypervisor with VM introspection techniques, we prefer to rely on a single entity to run the anti-virus software and the HIDS.

There are two main types of anti-virus software, signature based detection and behavior-based detection. In practice, the most common type of anti-virus software deployed is signature-based. Signature-based anti-virus software relies on the ability for someone to capture and analyze a malware sample and generate a rule or set of rules that we be able to reliable detect another sample found in the wild. It is a well known practice for malware writers to obfuscate their malware either by the use of encryption or padding with random bytes or no operations (NOPs) to attempt to evade standard signature-based software.

Alternatively, behavior-based ant-virus software attempts to build a general class of behavior-based signatures that will detect various attack classes by the types of operations performed regardless of whether the malware itself has been obfuscated. Behavior-based detection techniques are more challenging to develop and can lead to false positives (e.g. a user happens to perform steps that look like malware).

The combination of signature-based and behavior-based malware could be helpful in practice. Detailed discussion of these approaches are outside the scope of this dissertation. We will describe cases where anti-virus software could be a useful addition to our Rapid Recovery Desktop system in the evaluation section of Chapter 5.

Finally, an HIDS, such as tripwire\cite{kim_tripwire_1994}, is used as a way to keep track of all changes to files on the system. An HIDS will scan the entire system and generate some sort of hash or checksum for all the files and then compare those hashes or checksums to the files over time to watch for changes. If important system files changed unexpectedly, that could be a sign that malware has compromised the system. Virtualization support for a HIDS is very common in research. One very good example is a system called livewire\cite{VMI_IDS_2003} that uses virtual machine introspection techniques~\cite{xenaccess_07,vmsafe_news_2008} (or inspecting the internals of the VM from outside of it) to provide a sophisticated HIDS for a virtualization environment. Integrated a HIDS and/or using virtual machine introspection techniques would be an interesting area of future work.

Now that we have placed this work into its proper context and described the related work, we will describe the design of our system in the next chapter.



