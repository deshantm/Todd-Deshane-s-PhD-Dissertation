%  chapter-02.tex

\chapter{Related Work}

In this chapter, we discuss the related work that we build upon or complement. In order to place this work in the proper context, we first describe the scope of this work, including our basic threat model and assumptions. Some of the related research has similar goals to this work, but differs in terms of the threat model or basic assumptions. Other works have a similar similar threat models and assumptions, but are solving different problems. This chapter should provide an overview of how our work fits into that ecosystem.

\section{Scope of this Work}

This dissertation makes use of a variety of technologies and concepts. In short, we use paravirtualization and hardware-assisted virtualization of platforms virtual machines, along with network file servers and software virtual switches, to provide support for the principle of least privilege, access control, and isolation on end-user desktop systems using open source technologies.

\subsection{Virtualization Scope}

We focus on the two of the most common types of platform virtualization, namely paravirtulization and hardware-assisted full virtualization. We use paravirtulization since it provides high performance virtualization of open source operating systems, such as Linux. Paravirtualization can also make use of advances in hardware support for virtualization. Hardware-assisted virtualization is chosen because it provides efficient use of unmodified operating systems, which is helpful when working with proprietary guest operating systems, such as Microsoft Windows.

Hardware-assisted virtualization on commodity desktop systems has been commonly included since 2006, with the introduction of Intel VT~\cite{van_Doorn_2006}. Hardware vendors, such as AMD and Intel, and software vendors, such as Microsoft, VMware, Citrix, and Red Hat, have been cooperating in co-evolving the virtualization hardware and software to work better together in order to provide improved virtualization over time. This new co-evolution of commodity hardware and software for virtualization is similar to how IBM has co-evolved thier z/VM software with the zSeries mainframe hardware since the 1960s.

\subsection{Security Scope}

There are various security techniques that could be employed to enhance the security properties of our system, we focus primarily on the principle of least privilege, access control, and isolation. We choose to focus specifically on these security techniques since they get at the root of the problem that this dissertation attempts to solve, the problem of too much access and not enough isolation of applications.

\subsection{Environment Scope}

Our solution is primarily focused on providing security benefits to end-user desktop systems. Although our solution could also be applied to other environments, such as servers, mobile devices, cloud computing, and thin client virtual desktops (also known as desktop virtualization or virtual desktop infrastructure (VDI)), we consider detailed analysis of these other systems and environments outside the scope of this dissertation. Although we briefly touch on the applicability to other environments in the future work section of Chapter 6.

\subsection{Technology Scope}

We considered various virtualization and security technologies during the course of designing and implementing our prototype. Specifically, we try to focus on technologies that are open source and that follow open standards. Open source technologies are generally better for distribution, extensibility, modifiability, inspection, and promoting collaboration than proprietary technologies~\cite{ben-yehuda_osr_2008}. Technologies that follow open standards promote interoperability among other products, are more deployable in practice, and encourage collaboration among developers. Simply using open source and open standards does not necessarily lead to these benefits~\cite{crosby_blog_interoperability_2010,gigaom_blog_2010}, but we believe that they are a good first step.

Although our solution focuses on open source and open standards, proprietary technologies can often be interchanged with open source components. We do understand there are instances where vendors need to make use of proprietary technologies and therefore we make it possible for proprietary components to be supported in our system. We describe our overall design, including the decisions that allow elements of the system to be interchangeable, in Chapter 3.

\subsection{Threat Model and Assumptions}

We assume that the virtual machine monitor (hypervisor) is part of the trusted computing base (TCB) and is a small, simple software layer that is secure and not able to be compromised by an attacker. Hypervisor security and formal or cryptographic verification is outside the scope of this work. We also assume that several service VMs are part of the TCB and that we have taken reasonable measures to secure and isolate them from direct attack. In particularly, we include an administrative VM for basic VM management, a FS-VM for serving user data on a internal virtual network, and a NET-VM for managing the internal and external networks. We note that the administrative VM, FS-VM, and NET-VM could all be contained within a single VM (or included in the base operating system) for a severely resource limited system, but we recommend separating them to provide better isolation, modularity, and flexibility to the overall system. Finally, all VMs not included in the TCB are considered untrusted and could fall under full control of an attacker (detecting and recovering from this scenario is a fundamental goal of this work).

We make a conscious choice to support both Windows and Linux guest operating systems, but also do not make design decisions that would limit the possibility of supporting other guest operating systems.  We also choose to focus our support and testing on open source hypervisor solutions, but this is not a fundamental requirement. Finally, we also choose to support desktop systems as a priority, but consider application of our system to mobile devices (such as smart phones and tablets), server-class systems (including public, private, and hybrid cloud environments), web-based environments, and thin client setups to be important and natural extensions of this work.


%****possible addition that could be helpful***
%<summary table for threat models and/or basic assumptions as compared to this work>
%***


\section{Overview of Related Work}

We build upon and complement a rich array of related work. First, we consider the long history of virtualization, both on server-class systems, such as the IBM mainframe, and on commodity desktop systems. Lessons learned and concepts applied over the years have provided a great foundation on which this work lies. Next, we consider the long history of the field of computer security, primarily from a network and information security standpoint, and further limiting our scope to focus primarily on the factors that have a direct impact on commodity desktop computing. Then, we describe the broad spectrum of work that has combined virtualization and security techniques to address various security issues.

There is also a substantial amount of related work that shares some of the goals of our work. First, there has been lot of effort in the network security and network intrusion detection (NIDS) space that is often complementary to our NET-VM. Second there is a body of work in the backup and recovery space that is often complementary to our FS-VM. Finally, there is a set of related work that deals with malware and host-based intrusion detection systems (HIDS) that is often complementary to our system design as a whole.

The related work and concepts with respect to malware classification will be covered in evaluation in Chapter 5, since it will be more helpful to have that discussion available to evaluate effectiveness against attacks. Similarly, some of the more advanced concepts in the FS-VM related work will be included in the future work section of Chapter 6. Finally, there is a growing body of work in the area of human computer interaction (HCI) as it relates to security (SEC) in the specific field of HCI-SEC that will also be addressed in the future work section of Chapter 6.


\section{Virtualization}

\subsection{History and Evolution}

Virtual machine technology, including the virtual machine monitor (VMM) or hypervisor was pioneered by IBM in the 1960s~\cite{creasy_1981}. The original IBM VMM was designed for IBM System/370 machines and has co-evolved over the years with the IBM hardware that it runs on.  The current iteration of the IBM VMM is now known as the z/VM hypervisor and runs on IBM System z (zSeries) server hardware. Other VMM software and hardware co-evolutions include IBM's pSeries hypervisors for pSeries hardware~\cite{armstrong_power5_2005,blank_p5_2005} and Sun Microsystems Logical Domains (now Oracle VM for SPARC) for SPARC hardware~\cite{sparc_architecture_2005,sparc_vm_2006}. While, yet other hardware, such as the Alpha processor, was specifically designed to support virtualization \cite{karger_2007}. 

The history of virtual machine technology for mainstream PC platforms is an interesting one. Since x86 hardware was not designed to be virtualizable~\cite{popek_1974}, this introduced additional overhead and complexity in virtualization. Also, until recently, personal computers were not typically configured with sufficient memory to support multiple, simultaneously running VMs.  As PCs increased in power and memory prices fell, virtualization became more feasible for commodity platforms and a number of commercial and open source virtualization products were introduced. The Disco project~\cite{bugnion_1997} was the first to create a VMM that ran on experimental commodity ccNUMA hardware. Members of the Disco team later founded VMware, which is the commercial pioneer of virtual machine technology on x86 hardware~\cite{vmware_website,adams_2006}.
 
In this dissertation, we focus on two types of virtual machine technology: (1) Paravirtualization, which requires minor modifications to an operating system, making it aware that there is an underlying VMM and (2) Hardware-assisted full virtualization, which allows running unmodified operating systems on top of the VMM.

The first approach, paravirtualization, first coined by Denali~\cite{whitaker_2002} and then popularized by Xen~\cite{barham_2003}, brought with it evidence that virtualization benefits could be achieved with low overhead. With full virtualization, the guest VM is unaware that it is running on simulated hardware because the interface presented is the same as the physical hardware. With paravirtualization, however, the guest VM is aware that it is being virtualized since it is modified to make system (or hyper) calls directly into the hypervisor. The paravirtual modifications are usually small and are intended to improve performance by avoiding the use of the non-virtualizable instructions~\cite{popek_1974} and optimizing other operations. The paravirtualization approach has the advantage of better performance, but since some modification to the guest is required, it is ill-suited for use with closed source operating systems. When Xen released their performance numbers at SOSP 2003, a team of us at Clarkson University published independent verification of these results and extended the comparison to Linux running on an IBM zServer and also demonstrated that virtualization benefits could be realized on older hardware, with a low performance overhead~\cite{clark_2004}.
 
The second approach, hardware-assisted full virtualization, first showed up for commodity hardware in 2005 in the form of the Intel's VT-x virtualization extension, which was followed shortly after by AMD's AMD-V virtualization extensions~\cite{van_Doorn_2006}. This marked the beginning of a new era of virtualization software and hardware co-evolution. The co-evolution era, which we are still in the midst of, involves the cooperation of commodity market players with a variety of software hypervisors (such as Xen, KVM, and VMware) and hardware vendors (such as Intel and AMD). These virtualization hardware extensions allow for unmodified guest operating systems to run more effectively on a wider variety of virtualization platforms, such as Xen and KVM. The hardware extensions are required for full virtualization support (for example, Windows guests) on Xen and is requirement to use the Linux Kernel-based Virtual Machine (KVM)~\cite{kvm_ols07}.

First generation hardware support for virtualization (VT-x and AMD-V) made proper virtualization\cite{popek_1974} of the x86 hardware possible, but that did not guarantee performance gains compared to existing software approaches (such as binary re-writing) to virtualize the x86 architecture\cite{adams_2006}. The virtualization software and hardware co-evolution had only just begun. In an effort to shed some light on the initial mediocre performance of x86 hardware virtualization, Karger described some great performance and security lessons learned from virtualizating the Alpha processor and compared that architcture to x86 virtualization hardware\cite{karger_2007}. Karger explained that the simpler Alpha processor, based on a reduced instruction set computing (RISC) architecture, which was specifically designed to support virtualization had advantages, such as the way it handled sensitive instructions, page tables, and translation lookaside buffer (TLB) misses, that made it easier to implement high performance support for virtualization. Karger suggests that Intel and AMD should learn from the lessons of this and other architectures that were designed to support virtualization. The Karger paper and the general history of virtualization suggest that the co-evolution of hardware and software virtualization is a process that often needs to be refined over time (like for example the IBM z/VM and z Series) and that it is difficult for the hardware to be designed to support high performance virtualization from the beginning.

The software hypervisors, such as Xen and KVM, are evolving to make better use of the x86 hardware virtualization extensions. At the same time, the x86 hardware is evolving and vendors, such as Intel and AMD, have released second and third generation virtualization hardware extensions to add performance and security benefits. Second generation hardware extensions target performance improvements for switching between guest operation systems by adding hardware support for handling guest page tables (also referred to as shadow page tables). The specific technologies released are Intel Extended Page Tables (EPT) and AMD Nested Page Tables (NPT). Third generation hardware extensions, in the form of input/output memory management units (IOMMUs), seek to improve the security of virtual device direct memory access (DMA) and the performance of virtual I/O devices, such as graphics, disk, and network. Specific IOMMU hardware releases include Intel's VT-d and AMD's IOMMU. Other hardware virtualization technologies that are more targeted at businesses than consumers include Intel vPro and AMD DASH, which use on chip management capabilities and trusted platform module (TPM) technology to provide various security and manageability opportunities. The overall virtualization hardware and software co-evolution process is making virtualization a ubiquitous part of commodity computing both in the server and desktop markets. 

Further evidence that virtualization is making an impact on a wider audience is the XP mode feature that was added to Windows 7. This feature uses virtual machine technology to run Windows XP applications or a full Windows XP environment inside of Windows 7\cite{windows_xp_mode}. 

\subsection{Virtual Appliances}
\label{VirtualAppliances}

A more recent trend in the virtualization space is toward virtual machine appliances (or simply virtual appliances). Virtual appliances are pre-configured virtual machine instances that are specially designed for specific tasks. For example, appliances exist for user-level software, such as browsers, and server software, such as web and database servers. The ability to quickly deploy a pre-configured virtual appliance is a clear and compelling advantage of virtualization and is becoming an increasingly popular method for software distribution.

Virtual appliances, at a high level are analogous to household appliances that are used for one particular task. An even better analogy for virtual appliances is a comparison to information appliances. The term information appliance was coined by Jeff Raskin and was described in much detail in the book ``The Invisible Computer'' by Don Norman\cite{norman_1999}. The basic idea of information appliances is the concept that the Personal Computer (PC) is a general purpose device and since it tries to be everything to everyone, it fails at being usable. Further, the solution is to replace the PC with information appliances, or single purpose devices, such as digital cameras, printers, document writers, etc. that do one job and do it well. The idea is that special-purpose devices can be made to be much more usable. Information appliances together would then make up all the functions of the PC and the computer itself would become invisible (behind the scenes). Computers already play this role in part, but getting the computer industry to make the last big leap to a world of information appliances is a challenging one. ``The Invisible Computer'' goes into many aspects of the problem, from the market and business side of things to the complexities of large programming projects and operating systems.

Sapuntzakis et. al. first introduced the concept of a virtual appliance\cite{sapuntzakis_2003}, which they described as a virtual machine that replaces a physical computer appliance (such as a firewall). Their vision for virtual appliances was in the ``Collective'' architecture, which they described as a compute utility that provides virtual appliances as a service. Further, they explained that the virtual appliances would send their displays to a remote display on a thin client. Their concept is basically what we know of today as a cloud service that provides load balancing of infrastructure as a service (IaaS) or perhaps more closely analogous to desktop as a service (DaaS).

Virtual computer appliances were also mentioned as a component of the architecture proposed by Evanchik~\cite{evanchik_thesis_2004}. This was followed shortly after by our work in which we described how virtual machine appliances fit into a Rapid Recovery Desktop System\cite{rapid_recovery_paper_05}. The virtual machine appliance concept as proposed in our paper describes creating virtual appliances by placing one or more applications that have similar data and network access needs into a virtual machine. Further we recommended that appliances come with a virtual machine contract that explicitly specifies those needs.

VMware further popularized the virtual appliance concept with marketing and virtual appliance development contests with large cash prizes~\cite{herrod_keynote_2006,vmware_appliances_website}. Many other open source and commercial vendors are also distributing virtual appliances~\cite{rPath_website, stacklet_website, virtual_appliances_website, jumpbox_website}. The associated virtual machine contracts have not gained as much traction however. Virtual machines, and therefore also virtual appliances, often come with configuration files that specify the basic hardware needs (CPU, memory, disk, etc.) of the virtual machine, but there is still a need for contracts that allow for the configuration of more fine-grained data and network resource access needs. This dissertation presents a basic, extensible contract system implementation in order to address this need.

\subsection{Virtual Machine Contracts}
\label{VirtualMachineContracts}
To the best of our knowledge, the concept of virtual machine contracts (VMCs) in the context of virtual appliances was first introduced by Evanchik\cite{evanchik_thesis_2004}. The VMCs put forth in that thesis were based on the concept of having a virtual appliance specify a set of very specific system calls that it would be allowed to make. For example, any read or write system calls to files or directories would need to be specified. Further, network-based system calls, such as bind and listen, would need to be specified in the virtual appliance contract. The contract methodology of explicit allow and default deny is an approach that this dissertation builds upon. The contract enforcement proposed in that thesis was based on modifying the kernel of the virtual appliance and replacing system calls with hypercalls (system calls into the hypervisor) that are intercepted and validated by a contract enforcement element running in the hypervisor. 

Differences from that work and this dissertation are the contract specification and enforcement aspect. In this dissertation we implement the contract system in a much more general way. We are not limited to relying on enforcement within the kernel of the guest VM, as is proposed in the architecture proposed by Evanchik\cite{evanchik_thesis_2004}. Instead, we developed OSCKAR to give more flexibility and control to the virtual appliance and enforcement element designers. For example, any type of virtual appliance contract rules can be specified as long as there is an enforcement element that can respond to them. System call-based contracts could be employed with our OSCKAR system (provided that appropriate enforcement elements are implemented), but that contract style is not currently used in our current Rapid Recovery Desktop implementation. Chapters 3 and 4 present the design and implementation details of OSCKAR.

In\cite{rapid_recovery_paper_05}, our focus of the virtual machine contracts was on file system contract rules in which a dedicated file server virtual machine (FS-VM) stored user data and allowed virtual appliance to mount specific portions of the data in read, write, or append-only fashion. The FS-VM in that paper supported read and write rate-limiting with a modified NFS server. We extend the work done in that paper to add two new components, the OSCKAR virtualization security framework and the NET-VM.

In\cite{virtual_machine_contract_ICAC09}, Matthews et. al. proposed a contract system and architecture, including the concept of enforcement elements, very similar to the architecture presented in this paper. In that paper, they demonstrate the feasibility and approach of such a system in a data center environment and described extending the Open Virtualization Format (OVF)\cite{dmtf_newsletter}, which is an open standard for packaging and distributing virtual appliances, to support more advanced data and network access rules. We extend that work to a Rapid Recovery Desktop system and implement an open source virtualization security framework that supports custom contract rules and enforcement elements, which could include support for the OVF standard in the future.

\section{Security}

The security principles employed in this dissertation have been well-studied and applied in general, but we do suggest to use them, in combination with other technologies, such as virtualization, in a way that it not seen in common practice today. Specifically, we apply the principle of least privilege, isolation, and access control to virtual appliances.

\subsection{The Principle of Least Privilege}

A number of security principles were first formally describe by Saltzer and Schroeder in\cite{saltzer_1975}. Among those principles was the principle of least privilege, which states that ``Every program and every user of the system should operate using the least set of privileges necessary to complete the job''. Saltzer and Schroeder explain the rationale behind this principle in that it limits the damage that can occur from an accident or error, limits the interaction among privileged programs, and provides a rationale as to where to place protection. The goal of the system described in this dissertation is that virtual appliances adhere to the principle of least privilege as much as possible. We argue that strict adherence to the principle is likely to be impractical to implement in practice, since it would require perfect virtual appliance contracts, a very detail-level processing of virtual appliance operations, and would likely be intolerably hard to use for any user. As we will describe in the sections that follow, access control methods that attempt to apply the principle of least privilege to various degrees are often disabled by users. The fact that perfect adherence may not be possible is no excuse not to apply reasonable constraints on virtual appliances. Some examples of mechanisms that help to apply the principle of least privilege include isolation and access control, which we discuss in the next sections.

% discussion of these systems might be useful here
% histar, asbestos, flume etc.

\subsection{Isolation}

Complete isolation, as described in\cite{saltzer_1975}, is "a protection system that separates principals into compartments between which no flow of information or control is possible". Two approaches for achieving complete isolation described by Saltzer and Schroeder include isolated virtual machines and authentication mechanisms. Virtual machines, as described earlier in this chapter, have been used for many years on mainframe hardware and have, until the Disco project~\cite{bugnion_1997} in 1997 were considered computationally prohibitive on commodity systems. So, traditional isolation has relied on authentication mechanisms, such as username and password system login. This dissertation makes use of virtual machines to provide isolation between applications stored in virtual appliances. We are certainly not the first to make use of virtualization for this purpose, but are among a growing list of systems using virtualization for security purposes. Examples of other research systems will described later in this chapter. 

Although using virtual machines to provide isolation is very common in research, it is more challenging to apply virtualization to real production systems, especially on the desktop. As part of this dissertation we hope to encourage taking research ideas and converting them into real systems. This concept is exemplified by a recent alpha release of the Qubes operation system\cite{qubes-os_2010}, which is a new operating system based on the Xen hypervisor. We will describe Qubes in more detail in the Virtualization and Security section of this chapter, but for now we note that Qubes uses virtual machines and various virtualization hardware extensions to provide isolation for applications.

\subsection{Access Control}

\subsubsection{Discretionary Access Control}

Early mechanisms for access control were described by Lampson in\cite{lampson_accesscontrol_1974}. The principles and mechanisms described in that paper provide the foundation for the discretionary access control (DAC)\cite{sandhu_dac_1994} that is in common use today. The basic idea of DAC is an access control matrix with domains (users, groups, etc.) labeling the rows, objects (files, directories, processes, etc.) labeling the columns, and capabilities or access permissions (read, write, execute, etc.) as the entries within the matrix. The typically implementation is done with access control lists. One major weakness with DAC, as mentioned in Chapter 1, is that the granularity of access is too coarse. More specifically, if a user has access to a file, then any program running as that user has access to that file. Our approach to mitigating this problem is to apply access control at the virtual appliance level, specifically with the explicit allow, default deny policy (as was described in\cite{evanchik_thesis_2004}).

\subsubsection{Mandatory Access Control}

Recognizing the limitations of DAC are not a new revelation. One common alternative to DAC is mandatory access control (MAC), which is sometimes referred to as rule-based access control\cite{lindqvist_mac_2006} or lattice-based access control\cite{denning_mac_1976}. A traditional view of MAC associates it with multi-level security (MLS), but it has been recognized that the MLS-based approach is too limiting to meet many security requirements\cite{loscocco_2001}. The basic idea behind MAC is that interactions between subjects (users, programs, etc.) and objects (files, programs, etc) are handled by a set of system-wide security policies. The basic implementation is usually that all subjects and objects are labeled and policy logic is separated from the enforcement mechanism. MAC is significantly more sophisticated than DAC, but at a higher cost of complexity. 

The contract system presented in this dissertation shares many of the goals of MAC (for example, limiting user and application access, and separating mechanism from policy), but since our system is designed and implemented at the virtualization layer and applied to virtual appliances, we are able to specify resource restrictions at a higher level of abstraction. For example, we are able to write contract rules in terms of the virtual CPUs, memory, disks, and network resources of the virtual appliance. MAC policy, on the other hand, is typically specified in terms of lower level constructs, such as system calls and operation system objects (i.e. files and processes). Further, virtual appliances are likely to provide more isolation than MAC, since malware could potentially disable MAC that is running on a traditional operating system. However, malware within a virtual appliance would not be able to turn off our contract system unless it was somehow able to subvert the virtual machine by, for example, breaking out of the VM and into the hypervisor or breaking into a component of our trusted computing base. 

It is also worth mentioning the various implementations of MAC found in practice. These include SELinux~\cite{smalley_2001, loscocco_2001}, and AppArmor~\cite{AppArmor_2006}. Microsoft has also added a MAC model into its operating systems with the addition of Mandatory Integrity Control starting in Windows Vista~\cite{windows_integrity_mechanism, mandatory_integrity_control}. Another interesting policy enforcement tool, called Systrace\cite{provos_2003}, is touted as a lightweight replacement for MAC. This tool generates system call signatures in a learning mode and then enforces those policies in real time. A tool such as this could be used to generate system call-based contracts for applications. The output of such a tool could have been used directly with the implementation proposed in\cite{evanchik_thesis_2004}. Finally, SELinux Sandboxes~\cite{selinux_sandboxes_walsh_2009, selinux_sandboxes_morris_2009}, based on SELinux, are an attempt to further limit applications by making them run in a temporary sandbox directory that is clean after the application exits. SELinux-sandboxed applications can also be run within their own X server environment. 

Although MAC systems are becoming more powerful and easier to use, the most advanced features that they provide, such as SELinux Sandboxes, are generally only available for Linux applications. Using virtualization, as is described in this dissertation, allows applications from other operating systems, such as Windows, to be supported. MAC concepts and policies should also be considered complementary to our system for two reasons. First, existing MAC application policy rules (for example, the application-specific confinement rules that are written for existing MAC systems) could be used to help virtual appliance designers build better virtual appliance contracts. Second, MAC support could also be added to the virtualization layer of our system, the technologies that enabled this, sVirt\cite{sVirt_website} and XSM\cite{xsm_xen_summit_3rd}, will be discussed in the next section.

\section{Virtualization and Security}

There is a vast amount of related work that attempts to apply virtualization techniques to solve security problems. One of the main reasons for this is that the virtualization layer provides a unique perspective in relationship to the guest operating system. The VMM layer can be used to monitor the guest from below and, often times, without the guest OS knowing it is being watched\footnote{The "red pill" program can be used to detect if you are running in a virutal machine, see: ttp://invisiblethings.org/papers/redpill.html}. Some popular applications that make use of this unique perspective are intrusion detection systems~\cite{Panorama_07,VNIDA_08,VMFence_09,Psyco-Virt_07,Protecting_host_detectors_06,HyperSpector05, VMI_IDS_2003}, fault tolerance systems \cite{bressoud_1995}, virtual machine record and playback systems~\cite{dunlap_2002,king_2003}, malware analysis tools\cite{leet09_malware}, honeypots\cite{asrigo_2006}, secure desktop systems\cite{zhao_2005, Meushaw_2000, qubes-os_2010}, trusted computing platforms\cite{garfinkel_2003}, and sandboxes\cite{isolated_execution_2010}.

Some of the early work that used virtualization applied to security include the following. Bressoud and Schneider developed fault-tolerant systems using virtual machine technology to replicate the state of a primary system to a backup system\cite{bressoud_1995}. Dunlap et al. used virtual machines to provide secure logging and replay\cite{dunlap_2002}. King and Chen used virtual machine technology and secure logging to determine the cause of an attack after it has occurred\cite{king_2003}. Reed et al. used virtual machine technology to bring untrusted code safely into a shared computing environment\cite{reed_1999}. Zhao et al. used virtual machines to provide protection against root kits\cite{zhao_2005}.

\subsection{Virtualization and Isolation}

In this section we highlight two systems that use virtualization for the purpose of isolation of applications. The first system, called Isolated Execution\cite{isolated_execution_2010}, which has been released by Intel in alpha form as an open source sandbox system that allows a user to right click on a binary executable file and run it in a sandbox VM. Although the Isolated Execution system is in an early development stage, it does demonstrate useful concepts that could be applied to the system described in this dissertation.

The second system is an operating system, called Qubes\cite{qubes-os_2010}, that is built on top of the Xen hypervisor. At a high level, Qubes shares many of the components of our Rapid Recovery Desktop system. Specifically, they include a network domain, which is similar to our NET-VM component, and a storage domain, which is similar to our FS-VM component. The overall goal of their system, like ours, is to isolate desktop applications from each other using virtual appliances. However, their approach differs from ours is several interesting ways. First, their virtual appliances, which they refer to as AppVMs, are assumed to be based on a common base file system so as to be able to make use of a set of read-only core system files, which has the benefit of being able to do updates once to that shared system core. Due to this architectural choice, the current release only supports Linux as the base, but they are investigating ways to support other operating systems, such as Windows, in the future. Architecturally we choose to make a different choice for our Rapid Recovery Desktop system by creating virtual appliance that store their own system state, we are able to more easily support a variety of base operating systems.

Another difference in the Qubes architecture is that AppVMs store user data within the AppVMs themselves, which is in contrast to our Rapid Recovery Desktop system that stores user data in a dedicated FS-VM. This design choice exemplifies the different approaches in terms of recovery and threat model between the Qubes system and our Rapid Recovery Desktop system. The Qubes system uses the storage domain to store and backup user and application data in an encrypted file system, thus treating the storage domain as an untrusted entity and not part of the trusted computing base. Our system, on the other hand, uses the FS-VM to store user data and allows virtual appliances to mount specific parts of it. In this way, we treat our FS-VM as a part of our trusted computing base in order to provide an easy way to roll back virtual appliances without affected user data. Their threat model is specifically based around reducing the trusting computing base (they apply the concepts of disaggregation of the Xen management domain as described in\cite{murray_2008}), so that malware that compromises a particular component is not able to affect other parts of the system. In contrast, our threat model is based on distrusting virtual appliances, so that malware that compromises a virtual appliance is not able to compromise other appliances nor user data that is stored in a isolated, hardened, and carefully protected FS-VM. In Chapter 3, we describe the methods are architectural decisions we use to protect our FS-VM.

Similar to the storage domain in the Qubes architecture, the network domain is removed from the trusted computing base of their architecture and network policy enforcement is done within each of the AppVMs. Their reasoning for this goes back to their overall threat model concept of reducing the size of the trusted computing base and the assumption that having an external network component cannot provide additional security to a compromised AppVM. As before, our Rapid Recovery Desktop system architecture is in direct contrast to theirs in that we treat our NET-VM component as part of the trusted computing base and we use it to protect against malicious network activity, even in the case that a virtual appliance is compromised. We believe that by distrusting the virtual appliances, we can limit their ability to do harm to the rest of the system and the rest of the world. 

A final difference between the Qubes architecture and ours is that theirs relies on hardware support for virtualization. Specifically, they make use of the IOMMU support to give direct access to the network card to their network domain and the storage controller to their storage domain. Further they make use of the Trusted eXecution Technology (TXT) and the trusted platorm module (TPM) included in Intel's vPro to do crypographic signing of of boot and disk images. We plan to make use of the IOMMU capabilities for our NET-VM and FS-VM to improve performance and security, but we do not strictly rely on them to complete our threat model like Qubes does. This difference allows our system to be deployable on more hardware than Qubes.

Despite the differences in architecture between Qubes and our Rapid Recovery Desktop, there are still ways that we could make use of some of their techniques for specific use cases. We will describe aspects of the Qubes architecture that we would like to integrate in the future work section of Chapter 6.

\subsection{Virtualization and Access Control}

In the Mandatory Access Control (MAC) section earlier in this chapter we mentioned that MAC could be added to the virtualization layer. One interesting approach taken by Quynh et al.\cite{Quynh_2006} in their VMAC system was to add a special service VM that provides central management of MAC policies for other virtual machines. A VM like the one in that paper might be able to integrated, as future work, into our Rapid Recovery Desktop system.

MAC was added to the Xen hypervisor in the form of Xen Security Modules (XSM)\cite{xsm_xen_summit_3rd,xsm_xen_summit_4th}, which were implemented by the National Information Assurance Research Lab within the National Security Agency (NSA). XSM provide various MAC policies to be enforced at the Xen hypervisor level. MAC has also been integrated into the libvirt virtualization toolkit\cite{libvirt_website} in the form of sVirt\cite{sVirt_website}. As will be described in Chapter 4 on implementation, libvirt is used to interact with the various virtualization capabilities on Linux (and other OSes).  sVirt allows for MAC policy enforcement for the various virtualization systems that run on Linux, which does not yet (and may not necessarily ever completely) include support for Xen, since Xen is a stand alone hypervisor that is not intended to be integrated into Linux itself.

MAC policies at the hypervisor level could allow for much of the basic enforcement that our Rapid Recovery Desktop system needs along with various other more complicated scenarios. For example, it could be used to assign labels to VMs and enforce various policies, such as VM A is only allowed to run if VM B is not running, at the hypervisor level. Adding MAC support at the hypervisor level of the Rapid Recovery Desktop could be an interesting area of future work.

\section{Backup and Recovery}

In this section we consider the role backup and recovery systems could play with respect to our Rapid Recovery Desktop system. With our Rapid Recovery Desktop system, we focus on the problems of rapid system restoration and protection of user data. We are unaware of another system that has separated user data and system data in the way we are proposing and that has optimized the handling of each to provide rapid system restoration after an attack. Our Rapid Recovery Desktop system is not intended to be a replacement for making backups, but instead it should be considered complementary. Having backups is still required in the case of hardware failure, for example. 

Examples of how our Rapid Recovery Desktop system complements backup include the fact that it is much easier to find the important user data that should be backed up, since it is all stored in the FS-VM. Similarly, updates to virtual appliance state can be carefully checkpointed and backed up when crucial changes are made and rolled back to known good backups when compromised by malware. It is also the case that system restore facilities could still be used within the virtual appliances, but being able to restore a virtual appliance to a known good state with the Rapid Recovery Desktop system infrastructure may make some restore facilities obsolete in that restoring to an earlier trusted snapshot of a virtual machine is much quicker and easier than using a restore or reset facility.

Our system also helps streamline the backup process by allowing efforts to focus on the irreplaceable personal data rather than on the recoverable system data. It also allows backup efforts to be customized to the differing needs of system data and personal data. Specifically, there is a mismatch between the overall rate of change in system data and the user-visible rate of change.
 
System data changes at clearly predictable points (for example, when a new application is installed or a patch is applied). Between these points, new system data may be written (such as system logs), but often this activity is of little interest to users as long as the system continues to function. For example, if a month’s worth of system logs were lost, most users would be perfectly happy as long as the system was returned to an internally consistent and functioning state. Therefore, there is little need to protect this new system data between change points.
 
With user data, however, even small changes are important. For example, a user may only add 1 page of text to a report in an 8 hour workday, but the loss of that one day of data would be immediately visible. This means that efforts to protect user data can be effective even if targeted at a small percentage of overall data. Users also tend to retain a large body of personal data that is not actively being changed. Incremental backups can be kept much smaller when focused on changes to user data rather than system data.

One common approach to providing data protection and recovery from attack is making full backups of all data on the physical machine – both personal and system data. There are several ways to backup a system including copying all files to alternate media that can be mounted as a separate file system (for example, a data DVD) or making an exact bootable image of the drive with a utility such as Clonezilla\cite{clonezilla_website}.
 
Burning data to DVD or other removable media creates a portable backup that is well-suited to restoring personal data and transporting it to other systems. Mounting the backup is also an easy way to verify its correctness and completeness. However, backups of this type are rarely bootable and typically require system state to be restored via re-installation of the operating system and applications. For example, even if all of the files associated with a program are backed up, the program may still not run correctly from the backup (for example, if it requires registry changes, specific shared libraries or kernel support).
 
Making an exact image of the drive with a utility such as Clonezilla is a better way to backup system data. It maintains all dependencies between executables and the operating system. Images such as this can typically be either booted directly or used to re-image the damaged system to a bootable state. However, images such as this are not always portable to other systems as they may contain dependencies on the hardware configuration (such as CPU architecture). They are also not as convenient for mounting on other systems to extract individual files or to verify the completeness of the backup.
 
Despite the limitations of backup facilities, our system is designed to complement rather than replace backup. One goal of our system is to avoid the need for restoration from backup by preventing damage to personal data and providing rapid recovery of system data from known-good checkpoints. While it is still important to make backups, in many cases using our system's built in features can mean that users do not need to make use of their backups. Restoring a system from backups is often a cumbersome and manual process – not to mention an error-prone one. Given the small percentage of users that regularly backup their system (and the even smaller percentage that test the correctness of their backups), it is important to reduce the number of situations in which restoring from backup is required.
 
Our virtual machine appliances also make backups of system data that are portable to other machines. System data is made portable by the checkpoints of the virtual machine appliances. The virtualization system handles abstracting details of the underlying hardware platform so that guests will run on any machine.
 
When restoring a traditional system from a backup, users are typically forced to choose between returning their system to a usable state immediately or preserving the corrupted system for analysis of the failure or attack and to possibly recover data. With our architecture, users can save the corrupted system image while still immediately restoring a functional image. These images are also much smaller than full backups because they contain only system data, not personal data, such as a user’s music collection.
 
Finally, a key advantage of our system relative to backups is that our architecture allows compromised virtual machines to be restarted automatically and almost instantaneously. As soon as an intrusion is detected, the system can be restored to an uncompromised, fully functional system very quickly. Similar advantages can be achieved with network booting facilities, such as PXE, or system reset facilities like DeepFreeze\cite{deepfreeze_website}, especially if used in conjunction with personal data mounted from a separate physical file server. However, these solutions require access to server machines – the file server, the boot server that supplies new system images, the firewall, etc. In many ways, our architecture can be viewed as bringing the advantages of a managed LAN architecture with multiple machines to a single PC environment.

\section{Network Security and Intrusion Detection}

Similar to how backup and recovery systems are complementary to our Rapid Recovery Desktop system, standard network security and intrusion detection software can complement our system. With our current implementation we can already defend against a wide variety of attacks, so adding more network security software should be carefully considered. 

One type of network software that would likely make a lot of sense to integrate is a network intrusion detection system (NIDS), such as Snort\cite{roesch_1999} or Bro\cite{paxson_1999}. A NIDS can be configured to watch for attack signatures and other patterns. The main limitation with a NIDS is that new attacks, unless they contain a signature that can be recognized as malicious or follow a well known pattern, will not be noticed. Despite this limitation, a NIDS is an important component to integrate. In the Malware Classification section of Chapter 5 on evaluation, we will describe scenarios in which a NIDS could be helpful.

On the other hand, a traditional firewall might not be as necessary to integrate. Our NET-VM component can already block unknown flows from outside of the VMs at the virtual switch level. A firewall component, such as iptables\cite{iptables_website}, could be used either directly at the virtual network switch layer or on individual virtual machines for defense in depth. 

Another advantage of our flexible architecture is that enforcement elements, such as our NET-VM and other more traditional network enforcement (for example intrusion detection systems and firewalls) can be place either inside of our infrastructure or could be placed outside of our infrastructure in a separate server or the enforcement element service could be provided by a physical network appliance. The design details that allow for this type of flexibility will be discussed in more detail in the next chapter.

\section{Anti-Virus Software and Host-based Intrusion Detection Systems}

Anti-Malware software, which is referred to more commonly as anti-virus software, and Host-based Intrusion Detection Systems (HIDSes) can be used to complement the system as a whole. As we will describe in the Malware Classification section of Chapter 5 on evaluation, our system is able to defend quite well against a variety of attacks. However, there are still cases where a malware package and an HIDS could play a significant role. The advantage of virtualization is that malware software and an HIDS could be run outside of the individual virtual machines, so that malware would not be able to disable them even if it was able to obtain administrative privileges on the virtual appliance. Further, running anti-virus software or an HIDS on each virtual appliance would be a waste of resources. The defense in depth argument could be used, but since file systems can be mounted privately over the virtual network and the memory can be scanned from the hypervisor with VM introspection techniques, it is probably a simpler and more reasonable approach to simple rely on a single entity to run the anti-virus software and perhaps the same or another entity to run the HIDS.

There are two main types of anti-virus software, signature based detection and behavior-based detection. In practice, the most common type of malware software deployed is signature-based. Signature-based malware relies on the ability for someone to capture and analyze a malware sample and generate a rule or set of rules that we be able to reliable detect another sample found in the wild. It is a well known practice for malware writers to obfuscate their malware either by the use of encryption or padding with random bytes or no operations (NOPs) to attempt to evade standard signature-based software.

Alternatively, behavior-based ant-virus software attempts to build a general class of behavior-based signatures that will detect various attack classes by the types of operations performed regardless of whether the malware itself has been obfuscated. Behavior-based detection techniques are more challenging to develop and can lead to false positives (e.g. a user happens to perform steps that look like malware).

The combination of signature-based and behavior-based malware could be helpful in practice. Detailed discussion of these approaches are outside the scope of this dissertation. We will describe cases where anti-virus software could be a useful addition to our Rapid Recovery Desktop system in the evaluation section of Chapter 5.

Finally, an HIDS, such as tripwire\cite{kim_1994}, is used as a way to keep track of all changes to files on the system. An HIDS will scan the entire system and generate some sort of hash or checksum for all the files and then compare those hashes or checksums to the files over time to watch for changes. If important system files changed unexpectedly, that could be a sign that malware has compromised the system. Virtualization support for a HIDS is very common in research. One very good example is a system called livewire\cite{VMI_IDS_2003} that uses virtual machine introspection techniques~\cite{xenaccess_07,vmsafe_news_2008} (or inspecting the internals of the VM from outside of it) to provide a sophisticated HIDS for a virtualization environment. Integrated a HIDS and/or using virtual machine introspection techniques would be an interesting area of future work.

Now that we have placed this work into its proper context and described the related work, we will describe the design of our system in the next chapter.


